{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Q-learner Agent\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 473,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "from collections import namedtuple\n",
    "import random\n",
    "from typing import Callable\n",
    "from copy import deepcopy\n",
    "from itertools import accumulate\n",
    "from operator import xor\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 474,
   "metadata": {},
   "outputs": [],
   "source": [
    "NIM_SIZE = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 475,
   "metadata": {},
   "outputs": [],
   "source": [
    "Nimply = namedtuple(\"Nimply\", \"row, num_objects\") # move"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 476,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Nim:\n",
    "    def __init__(self, num_rows: int, k: int = None) -> None:\n",
    "        self._rows = [i * 2 + 1 for i in range(num_rows)]\n",
    "        self._k = k\n",
    "\n",
    "    def __bool__(self):\n",
    "        return sum(self._rows) > 0\n",
    "\n",
    "    def __str__(self):\n",
    "        return \"<\" + \" \".join(str(_) for _ in self._rows) + \">\"\n",
    "\n",
    "    @property\n",
    "    def rows(self) -> tuple:\n",
    "        return tuple(self._rows)\n",
    "\n",
    "    @property\n",
    "    def k(self) -> int:\n",
    "        return self._k\n",
    "\n",
    "    def nimming(self, ply: Nimply) -> None:\n",
    "        row, num_objects = ply\n",
    "        assert self._rows[row] >= num_objects\n",
    "        assert self._k is None or num_objects <= self._k\n",
    "        self._rows[row] -= num_objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 477,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pure_random(state: Nim) -> Nimply:\n",
    "    \"\"\"A strategy that returns a random possible move\"\"\"\n",
    "    row = random.choice([r for r, c in enumerate(state.rows) if c > 0])\n",
    "    num_objects = random.randint(1, state.rows[row])\n",
    "    return Nimply(row, num_objects)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 478,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nim_sum(state: Nim) -> int:\n",
    "    \"\"\"Calculates the nim-sum of the board in a given state\"\"\"\n",
    "    *_, result = accumulate(state.rows, xor)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 479,
   "metadata": {},
   "outputs": [],
   "source": [
    "def active_rows_index(state: Nim) -> list:\n",
    "  \"\"\"Returns a list with the index of all the active rows(rows with elem > 0)\"\"\"\n",
    "  active_rows_index = []\n",
    "  count = 0\n",
    "  for o in state.rows:\n",
    "      if o > 0:\n",
    "        active_rows_index.append(count)\n",
    "      count += 1\n",
    "  return active_rows_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 480,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cook_status(state: Nim) -> dict:\n",
    "    \"\"\" \"\"\"\n",
    "    cooked = dict()\n",
    "    cooked[\"possible_moves\"] = [\n",
    "        (r, o) for r, c in enumerate(state.rows) for o in range(1, c + 1) if state.k is None or o <= state.k\n",
    "    ]\n",
    "    cooked[\"active_rows_number\"] = sum(o > 0 for o in state.rows)\n",
    "\n",
    "    #list with all active rows\n",
    "    cooked[\"active_rows_index\"] = active_rows_index(state)\n",
    "\n",
    "    #list of rows with only 1 elem\n",
    "    cooked[\"rows_with_one_element\"] = [(index, r) for index, r in enumerate(state.rows) if r == 1]\n",
    "\n",
    "    #list of rows with multiple elem\n",
    "    cooked[\"rows_multiple_elem\"] = [(index, r) for index, r in enumerate(state.rows) if r > 1]\n",
    "\n",
    "    #index of the shortest row\n",
    "    cooked[\"shortest_row\"] = min((x for x in enumerate(state.rows) if x[1] > 0), key=lambda y: y[1])[0]\n",
    "\n",
    "    #index of the longest row\n",
    "    cooked[\"longest_row\"] = max((x for x in enumerate(state.rows)), key=lambda y: y[1])[0]\n",
    "\n",
    "    cooked[\"nim_sum\"] = nim_sum(state)\n",
    "    cooked[\"pure_random\"] = pure_random(state)\n",
    "\n",
    "    brute_force = list()\n",
    "    for m in cooked[\"possible_moves\"]:\n",
    "        tmp = deepcopy(state)\n",
    "        tmp.nimming(m)\n",
    "        brute_force.append((m, nim_sum(tmp)))\n",
    "    cooked[\"brute_force\"] = brute_force\n",
    "\n",
    "    return cooked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 481,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_agent(state: Nim):\n",
    "    \"\"\"A strategy returning a random possible move\"\"\"\n",
    "    data = cook_status(state)\n",
    "    return data[\"pure_random\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 482,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_smart_agent(state: Nim):\n",
    "    \"\"\"A strategy returning a random possible move\"\"\"\n",
    "    data = cook_status(state)\n",
    "    if (data[\"active_rows_number\"] == 1):\n",
    "        row = data[\"active_rows_index\"][0]\n",
    "        elem = state.rows[row]\n",
    "        ply = Nimply(row, elem)\n",
    "    else:\n",
    "        ply = data[\"pure_random\"]\n",
    "    return ply"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 483,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hard_coded_agent(state: Nim):\n",
    "    \"\"\"Agent using fixed rules\"\"\"\n",
    "    data = cook_status(state=state)\n",
    "\n",
    "    active_rows_number = data[\"active_rows_number\"]\n",
    "    active_rows_index = data[\"active_rows_index\"]\n",
    "    rows_with_multiple_elem = data[\"rows_multiple_elem\"]\n",
    "    longest_row = data[\"longest_row\"]\n",
    "\n",
    "    if active_rows_number == 1:\n",
    "        row = active_rows_index[0]\n",
    "        elem = state.rows[row]\n",
    "        \n",
    "    elif active_rows_number % 2 == 0:\n",
    "        if len(rows_with_multiple_elem) == 1: \n",
    "            row = rows_with_multiple_elem[0][0]\n",
    "            elem = rows_with_multiple_elem[0][1] - 1 # take all elem exept one\n",
    "            logging.debug(f\"Even rows one mul, elem: {elem}\") \n",
    "        else:\n",
    "            row = longest_row\n",
    "            logging.debug(f\"longest row index: {longest_row}, elem: {state.rows[longest_row]}\")\n",
    "            elem = max(state.rows[longest_row] - 1, 1) # take all elem exept one\n",
    "            logging.debug(f\"Even rows, several mul, elem: {elem}\") \n",
    "    else:\n",
    "        if len(rows_with_multiple_elem) == 1:\n",
    "            row = rows_with_multiple_elem[0][0]\n",
    "            elem = rows_with_multiple_elem[0][1] # take all elem\n",
    "            logging.debug(f\"Odd rows, one mul, elem: {elem}\") \n",
    "        else:\n",
    "            row = longest_row\n",
    "            elem = state.rows[longest_row]\n",
    "            logging.debug(f\"Even rows, several mul, elem: {elem}\") \n",
    "\n",
    "    ply = Nimply(row, elem)\n",
    "    return ply"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 484,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimal_startegy(state: Nim) -> Nimply:\n",
    "    \"\"\"A strategy using nim sum to return the optimal move\"\"\"\n",
    "    data = cook_status(state)\n",
    "    return next((bf for bf in data[\"brute_force\"] if bf[1] == 0), random.choice(data[\"brute_force\"]))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 485,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dumb(state: Nim):\n",
    "    \"\"\"A dumb strategy that always picks one element from the longest row\"\"\"\n",
    "    data = cook_status(state=state)\n",
    "    row = data[\"longest_row\"]\n",
    "    \n",
    "    return Nimply(row, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 486,
   "metadata": {},
   "outputs": [],
   "source": [
    "OPPONENTS = [dumb, hard_coded_agent, random_agent, random_smart_agent, optimal_startegy]\n",
    "#Improvement: play against more agents with wider veriety of level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 487,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QLearner:\n",
    "    #learning rate and discount factor\n",
    "\n",
    "    REWARD = 1\n",
    "    PENALTY = -1\n",
    "    previous_state = None\n",
    "    previous_move = None\n",
    "\n",
    "    def __init__(self, learning_rate, discount_rate, exploration_rate):\n",
    "       q = {} # (state_rows: list, move: tuple (row, elem)) -> value: int\n",
    "       self.q = q\n",
    "        \n",
    "       #in a deterministic environment, the optimal learning rate is 1\n",
    "       #in practice, often a constant learning rate is used\n",
    "       self.learning_rate = learning_rate\n",
    "       #starting with a lower discont factor and increasing it towards its final value acelerates learning\n",
    "       self.discount_rate = discount_rate\n",
    "       #try to reduce the exploration rate while we are training the q-learner \n",
    "       self.exploration_rate = exploration_rate\n",
    "    \n",
    "    def clear_previous_vars(self):\n",
    "      self.previous_state = None\n",
    "      self.previous_move = None\n",
    "    \n",
    "    def change_exploration_rate(self, new_exploration_rate):\n",
    "      self.exploration_rate = new_exploration_rate\n",
    "\n",
    "    def change_discount_rate(self, new_discount_rate):\n",
    "      self.discount_rate = new_discount_rate\n",
    "\n",
    "    def change_learning_rate(self, new_learning_rate):\n",
    "      self.learning_rate = new_learning_rate\n",
    "\n",
    "    def add_state_moves(self, current_state): #function to add new state, moves combinations\n",
    "      \n",
    "      data = cook_status(current_state)\n",
    "      possible_moves = data['possible_moves']\n",
    "\n",
    "      for move in possible_moves:\n",
    "        if (current_state.rows, move) not in self.q: #adds the combination state, move to the q\n",
    "          self.q[(current_state.rows, move)] = np.random.uniform(0.0,0.01) #attribute a small random value \n",
    "    \n",
    "    #gets the move to apply\n",
    "    def policy(self, current_state):\n",
    "\n",
    "      data = cook_status(current_state)\n",
    "      possible_moves = data['possible_moves']\n",
    "\n",
    "      if np.random.random() > self.exploration_rate:\n",
    "\n",
    "        #we want to return the action with the biggest value\n",
    "        q_val_list = [self.q[(current_state.rows, move)] for move in possible_moves] #list of the values of state and action\n",
    "        max_val_index = np.argmax(q_val_list) #returns the index of the max element of the array \n",
    "        return possible_moves[max_val_index]  #returns the move with the biggest q_value\n",
    "\n",
    "      else: #we explore\n",
    "        return random.sample(possible_moves, 1)[0] #returns a random possible move - moves are in tuples\n",
    "    \n",
    "    def updateQ(self, current_state): #current_state: Nim\n",
    "\n",
    "      if not current_state: #if the game is finished\n",
    "        self.q[(self.previous_state, self.previous_move)] += \\\n",
    "                self.learning_rate * (self.PENALTY - self.q[(self.previous_state, self.previous_move)])\n",
    "        current_move = self.previous_state = self.previous_move = None #clear in order to prepare for the next game\n",
    "\n",
    "      else: #if the game is not finished \n",
    "        self.add_state_moves(current_state) #adds the new state, moves\n",
    "        current_move = self.policy(current_state) #gets the move that we want to use\n",
    "\n",
    "        if self.previous_move is not None: #if it is not the first move\n",
    "          next_state = deepcopy(current_state) #current_state: Nim\n",
    "          next_state.nimming(Nimply(current_move[0], current_move[1])) #get the next state applying the move (result of your move)\n",
    "\n",
    "          reward = 0 if next_state else self.REWARD #gets the value of the reward, if it wins, reward = 1\n",
    "          logging.debug(f\" REWARD: {reward}\")\n",
    "          data = cook_status(current_state)\n",
    "          possible_moves = data['possible_moves']\n",
    "\n",
    "          maxQ = max([self.q[(current_state.rows, move)] for move in possible_moves]) #max qvalue from the possible moves of the current_state\n",
    "\n",
    "          \n",
    "          self.q[(self.previous_state, self.previous_move)] += \\\n",
    "                    self.learning_rate * (reward + (self.discount_rate * maxQ) - \\\n",
    "                    self.q[(self.previous_state, self.previous_move)]) \n",
    "      \n",
    "\n",
    "        self.previous_state, self.previous_move = current_state.rows, current_move\n",
    "        logging.debug(f\"current_move - game not finished: {current_move}\")\n",
    "      return current_move\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 488,
   "metadata": {},
   "outputs": [],
   "source": [
    "def play_q_learning(nim_size, q_learner, external_agent): #plays the game once\n",
    "  nim = Nim(nim_size) #creates nim\n",
    "\n",
    "  #q-learner is the first player\n",
    "  #second player is the external agent - can be either dumb, random, optimizer\n",
    "\n",
    "  game_on = True #bool that is true while the game is happening\n",
    "  is_q_learner = True #we start with q-learner\n",
    "\n",
    "  while game_on:\n",
    "\n",
    "    if is_q_learner: #if the current player is our q_learner\n",
    "        move_params = q_learner.updateQ(nim)\n",
    "        logging.debug(f\" Wanted move after player = q-learner: {move_params}, State before move: {nim}\")\n",
    "        \n",
    "        if(move_params == None): #if q_learner loses\n",
    "            logging.debug(f\" Q-learner lost\")\n",
    "            return \"q_learner lost\"\n",
    "        \n",
    "        move_to_apply = Nimply(move_params[0], move_params[1])\n",
    "        logging.debug(f\"move to apply: {move_to_apply}\")\n",
    "        nim.nimming(move_to_apply)\n",
    "        \n",
    "        logging.debug(f\" <<NIM>> after q-learner move: {nim}\")\n",
    "        \n",
    "        if(sum(nim.rows) == 0): #if q_learner wins\n",
    "            logging.debug(f\"Q-learner won\")\n",
    "            q_learner.clear_previous_vars()\n",
    "            \n",
    "            return \"q_learner won\"\n",
    "        \n",
    "        is_q_learner = False\n",
    "    \n",
    "    else: #if the current player is the external agent\n",
    "        move_to_apply = external_agent(nim) \n",
    "        logging.debug(f\" Agent move to apply: {move_to_apply}\")\n",
    "        nim.nimming(move_to_apply)\n",
    "        is_q_learner = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 489,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "logging.getLogger().setLevel(logging.INFO)\n",
    "\n",
    "def q_learner_strategy(nim_size) -> QLearner: #function to train the q_learner\n",
    "  #q_learner will play against dumb, random, optimizer\n",
    "  num_games = 200\n",
    "  #num_games = 50 when nim_size = 10\n",
    "  current_explorration_rate = 0.6\n",
    "  q_learner_agent = QLearner(learning_rate=0.9, discount_rate=0.4, exploration_rate=current_explorration_rate) #change this later\n",
    "\n",
    "  for opponent in OPPONENTS:\n",
    "\n",
    "    for game in range(num_games):\n",
    "        play_q_learning(nim_size, q_learner_agent, opponent)\n",
    "        logging.debug(f\" GAME FINISHED\")\n",
    "    \n",
    "    current_explorration_rate -= 0.10\n",
    "\n",
    "    if (current_explorration_rate < 0.1):\n",
    "      current_explorration_rate = 0.1\n",
    "    \n",
    "    q_learner_agent.change_exploration_rate(current_explorration_rate)\n",
    "\n",
    "    num_games += 2*(game+1)\n",
    "    print(f\"NUM_GAMES\", num_games)\n",
    "  return q_learner_agent\n",
    "\n",
    "# improvement: practice agains each optiment until 100% winning rate, then move on to the next better opponent until reach optimal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 490,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NUM_GAMES 600\n",
      "NUM_GAMES 1800\n",
      "NUM_GAMES 5400\n",
      "NUM_GAMES 16200\n",
      "NUM_GAMES 48600\n"
     ]
    }
   ],
   "source": [
    "strat = q_learner_strategy(NIM_SIZE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 491,
   "metadata": {},
   "outputs": [],
   "source": [
    "def play_nim(q_learner: QLearner, opponent: Callable):\n",
    "\n",
    "    result = play_q_learning(NIM_SIZE, q_learner, opponent)\n",
    "    print(result)\n",
    "\n",
    "# Create evoultion agent\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 492,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "q_learner won\n"
     ]
    }
   ],
   "source": [
    "play_nim(strat, optimal_startegy)\n",
    "\n",
    "#test = Nim(4)\n",
    "#test.nimming(Nimply(2,1))\n",
    "#test.nimming(Nimply(3,4))\n",
    "\n",
    "#print(optimal_startegy(test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 493,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "q_learner won\n",
      "q_learner won\n",
      "q_learner won\n",
      "q_learner won\n",
      "q_learner won\n",
      "q_learner won\n",
      "q_learner won\n",
      "q_learner won\n",
      "q_learner won\n",
      "q_learner lost\n",
      "q_learner lost\n",
      "q_learner won\n",
      "q_learner won\n",
      "q_learner won\n",
      "q_learner won\n",
      "q_learner won\n",
      "q_learner won\n",
      "q_learner won\n",
      "q_learner won\n",
      "q_learner won\n",
      "q_learner won\n",
      "q_learner won\n",
      "q_learner won\n",
      "q_learner won\n",
      "q_learner won\n",
      "q_learner won\n",
      "q_learner won\n",
      "q_learner won\n",
      "q_learner won\n",
      "q_learner won\n",
      "q_learner won\n",
      "q_learner won\n",
      "q_learner won\n",
      "q_learner won\n",
      "q_learner won\n",
      "q_learner won\n",
      "q_learner won\n",
      "q_learner lost\n",
      "q_learner won\n",
      "q_learner won\n",
      "q_learner won\n",
      "q_learner won\n",
      "q_learner won\n",
      "q_learner won\n",
      "q_learner lost\n",
      "q_learner won\n",
      "q_learner won\n",
      "q_learner won\n",
      "q_learner won\n",
      "q_learner won\n",
      "q_learner won\n",
      "q_learner won\n",
      "q_learner won\n",
      "q_learner won\n",
      "q_learner won\n",
      "q_learner won\n",
      "q_learner won\n",
      "q_learner won\n",
      "q_learner won\n",
      "q_learner lost\n",
      "q_learner won\n",
      "q_learner won\n",
      "q_learner won\n",
      "q_learner won\n",
      "q_learner won\n",
      "q_learner won\n",
      "q_learner lost\n",
      "q_learner won\n",
      "q_learner won\n",
      "q_learner won\n",
      "q_learner won\n",
      "q_learner won\n",
      "q_learner won\n",
      "q_learner lost\n",
      "q_learner won\n",
      "q_learner won\n",
      "q_learner won\n",
      "q_learner won\n",
      "q_learner won\n",
      "q_learner won\n",
      "q_learner won\n",
      "q_learner won\n",
      "q_learner lost\n",
      "q_learner won\n",
      "q_learner won\n",
      "q_learner won\n",
      "q_learner lost\n",
      "q_learner won\n",
      "q_learner won\n",
      "q_learner won\n",
      "q_learner won\n",
      "q_learner won\n",
      "q_learner lost\n",
      "q_learner won\n",
      "q_learner won\n",
      "q_learner won\n",
      "q_learner won\n",
      "q_learner won\n",
      "q_learner won\n",
      "q_learner won\n"
     ]
    }
   ],
   "source": [
    "for _ in range(100):\n",
    "    play_nim(strat, random_smart_agent)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 64-bit (microsoft store)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "8f2b8f7d7da3b55c8640ff0ad5b752ba61ffdffe564a4378c820bcd9964834b8"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
