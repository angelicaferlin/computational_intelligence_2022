{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Q-learner Agent\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "from collections import namedtuple\n",
    "import random\n",
    "from typing import Callable\n",
    "from copy import deepcopy\n",
    "from itertools import accumulate\n",
    "from operator import xor\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "Nimply = namedtuple(\"Nimply\", \"row, num_objects\") # move"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Nim:\n",
    "    def __init__(self, num_rows: int, k: int = None) -> None:\n",
    "        self._rows = [i * 2 + 1 for i in range(num_rows)]\n",
    "        self._k = k\n",
    "\n",
    "    def __bool__(self):\n",
    "        return sum(self._rows) > 0\n",
    "\n",
    "    def __str__(self):\n",
    "        return \"<\" + \" \".join(str(_) for _ in self._rows) + \">\"\n",
    "\n",
    "    @property\n",
    "    def rows(self) -> tuple:\n",
    "        return tuple(self._rows)\n",
    "\n",
    "    @property\n",
    "    def k(self) -> int:\n",
    "        return self._k\n",
    "\n",
    "    def nimming(self, ply: Nimply) -> None:\n",
    "        row, num_objects = ply\n",
    "        assert self._rows[row] >= num_objects\n",
    "        assert self._k is None or num_objects <= self._k\n",
    "        self._rows[row] -= num_objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pure_random(state: Nim) -> Nimply:\n",
    "    \"\"\"A strategy that returns a random possible move\"\"\"\n",
    "    row = random.choice([r for r, c in enumerate(state.rows) if c > 0])\n",
    "    num_objects = random.randint(1, state.rows[row])\n",
    "    return Nimply(row, num_objects)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "NIM_SIZE = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nim_sum(state: Nim) -> int:\n",
    "    \"\"\"Calculates the nim-sum of the board in a given state\"\"\"\n",
    "    *_, result = accumulate(state.rows, xor)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "def active_rows_index(state: Nim) -> list:\n",
    "  \"\"\"Returns a list with the index of all the active rows(rows with elem > 0)\"\"\"\n",
    "  active_rows_index = []\n",
    "  count = 0\n",
    "  for o in state.rows:\n",
    "      if o > 0:\n",
    "        active_rows_index.append(count)\n",
    "      count += 1\n",
    "  return active_rows_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cook_status(state: Nim) -> dict:\n",
    "    \"\"\" \"\"\"\n",
    "    cooked = dict()\n",
    "    cooked[\"possible_moves\"] = [\n",
    "        (r, o) for r, c in enumerate(state.rows) for o in range(1, c + 1) if state.k is None or o <= state.k\n",
    "    ]\n",
    "    cooked[\"active_rows_number\"] = sum(o > 0 for o in state.rows)\n",
    "\n",
    "    #list with all active rows\n",
    "    cooked[\"active_rows_index\"] = active_rows_index(state)\n",
    "\n",
    "    #list of rows with only 1 elem\n",
    "    cooked[\"rows_with_one_element\"] = [(index, r) for index, r in enumerate(state.rows) if r == 1]\n",
    "\n",
    "    #list of rows with multiple elem\n",
    "    cooked[\"rows_multiple_elem\"] = [(index, r) for index, r in enumerate(state.rows) if r > 1]\n",
    "\n",
    "    #index of the shortest row\n",
    "    cooked[\"shortest_row\"] = min((x for x in enumerate(state.rows) if x[1] > 0), key=lambda y: y[1])[0]\n",
    "\n",
    "    #index of the longest row\n",
    "    cooked[\"longest_row\"] = max((x for x in enumerate(state.rows)), key=lambda y: y[1])[0]\n",
    "\n",
    "    cooked[\"nim_sum\"] = nim_sum(state)\n",
    "    cooked[\"pure_random\"] = pure_random(state)\n",
    "\n",
    "    brute_force = list()\n",
    "    for m in cooked[\"possible_moves\"]:\n",
    "        tmp = deepcopy(state)\n",
    "        tmp.nimming(m)\n",
    "        brute_force.append((m, nim_sum(tmp)))\n",
    "    cooked[\"brute_force\"] = brute_force\n",
    "\n",
    "    return cooked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_agent(state: Nim):\n",
    "    \"\"\"A strategy returning a random possible move\"\"\"\n",
    "    data = cook_status(state)\n",
    "    return data[\"pure_random\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimal_startegy(state: Nim) -> Nimply:\n",
    "    \"\"\"A strategy using nim sum to return the optimal move\"\"\"\n",
    "    data = cook_status(state)\n",
    "    return next((bf for bf in data[\"brute_force\"] if bf[1] == 0), random.choice(data[\"brute_force\"]))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dumb(state: Nim):\n",
    "    \"\"\"A dumb strategy that always picks one element from the longest row\"\"\"\n",
    "    data = cook_status(state=state)\n",
    "    row = data[\"longest_row\"]\n",
    "    \n",
    "    return Nimply(row, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "OPPONENTS = [dumb]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QLearner:\n",
    "    #learning rate and discount factor\n",
    "\n",
    "    REWARD = 1\n",
    "    PENALTY = -1\n",
    "    previous_state = None\n",
    "    previous_move = None\n",
    "\n",
    "    def __init__(self, learning_rate, discount_rate, exploration_rate):\n",
    "       q = {} # (state_rows: list, move: tuple (row, elem)) -> value: int\n",
    "       self.q = q\n",
    "        \n",
    "       #in a deterministic environment, the optimal learning rate is 1\n",
    "       #in practice, often a constant learning rate is used\n",
    "       self.learning_rate = learning_rate\n",
    "       #starting with a lower discont factor and increasing it towards its final value acelerates learning\n",
    "       self.discount_rate = discount_rate\n",
    "       #try to reduce the exploration rate while we are training the q-learner \n",
    "       self.exploration_rate = exploration_rate\n",
    "    \n",
    "    def clear_previous_vars(self):\n",
    "      self.previous_state = None\n",
    "      self.previous_move = None\n",
    "\n",
    "    def add_state_moves(self, current_state): #function to add new state, moves combinations\n",
    "      \n",
    "      data = cook_status(current_state)\n",
    "      possible_moves = data['possible_moves']\n",
    "\n",
    "      for move in possible_moves:\n",
    "        if (current_state.rows, move) not in self.q: #adds the combination state, move to the q\n",
    "          self.q[(current_state.rows, move)] = np.random.uniform(0.0,0.01) #attribute a small random value \n",
    "    \n",
    "    #gets the move to apply\n",
    "    def policy(self, current_state):\n",
    "\n",
    "      data = cook_status(current_state)\n",
    "      possible_moves = data['possible_moves']\n",
    "\n",
    "      if np.random.random() > self.exploration_rate:\n",
    "\n",
    "        #we want to return the action with the biggest value\n",
    "        q_val_list = [self.q[(current_state.rows, move)] for move in possible_moves] #list of the values of state and action\n",
    "        max_val_index = np.argmax(q_val_list) #returns the index of the max element of the array \n",
    "        return possible_moves[max_val_index]  #returns the move with the biggest q_value\n",
    "\n",
    "      else: #we explore\n",
    "        return random.sample(possible_moves, 1)[0] #returns a random possible move - moves are in tuples\n",
    "    \n",
    "    def updateQ(self, current_state): #current_state: Nim\n",
    "\n",
    "      if not current_state: #if the game is finished\n",
    "        self.q[(self.previous_state, self.previous_move)] += \\\n",
    "                self.learning_rate * (self.PENALTY - self.q[(self.previous_state, self.previous_move)]) #TODO - check this formula\n",
    "        current_move = self.previous_state = self.previous_move = None #clear in order to prepare for the next game\n",
    "\n",
    "      else: #if the game is not finished - TODO: review this last part - understand it well\n",
    "        self.add_state_moves(current_state) #adds the new state, moves\n",
    "        current_move = self.policy(current_state) #gets the move that we want to use\n",
    "\n",
    "        if self.previous_move is not None: #if it is not the first move\n",
    "          next_state = deepcopy(current_state) #current_state: Nim\n",
    "          next_state.nimming(Nimply(current_move[0], current_move[1])) #get the next state applying the move (result of your move)\n",
    "\n",
    "          reward = 0 if next_state else self.REWARD #gets the value of the reward, if it wins, reward = 1\n",
    "          logging.debug(f\" REWARD: {reward}\")\n",
    "          data = cook_status(current_state)\n",
    "          possible_moves = data['possible_moves']\n",
    "\n",
    "          maxQ = max([self.q[(current_state.rows, move)] for move in possible_moves]) #max qvalue from the possible moves of the current_state\n",
    "\n",
    "          \n",
    "          self.q[(self.previous_state, self.previous_move)] += \\\n",
    "                    self.learning_rate * (reward + (self.discount_rate * maxQ) - \\\n",
    "                    self.q[(self.previous_state, self.previous_move)]) # TODO - check this formula\n",
    "      \n",
    "\n",
    "        self.previous_state, self.previous_move = current_state.rows, current_move\n",
    "        logging.debug(f\"current_move - game not finished: {current_move}\")\n",
    "      return current_move\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "def play_q_learning(nim_size, q_learner, external_agent): #plays the game once\n",
    "  nim = Nim(nim_size) #creates nim\n",
    "\n",
    "  #q-learner is the first player\n",
    "  #second player is the external agent - can be either dumb, random, optimizer\n",
    "\n",
    "  game_on = True #bool that is true while the game is happening\n",
    "  is_q_learner = True #we start with q-learner\n",
    "\n",
    "  while game_on:\n",
    "\n",
    "    if is_q_learner: #if the current player is our q_learner\n",
    "        move_params = q_learner.updateQ(nim)\n",
    "        logging.debug(f\" Wanted move after player = q-learner: {move_params}, State before move: {nim}\")\n",
    "        \n",
    "        if(move_params == None): #if q_learner loses\n",
    "            logging.debug(f\" Q-learner lost\")\n",
    "            return \"q_learner lost\"\n",
    "        \n",
    "        move_to_apply = Nimply(move_params[0], move_params[1])\n",
    "        logging.debug(f\"move to apply: {move_to_apply}\")\n",
    "        nim.nimming(move_to_apply)\n",
    "        \n",
    "        logging.debug(f\" <<NIM>> after q-learner move: {nim}\")\n",
    "        \n",
    "        if(sum(nim.rows) == 0): #if q_learner wins\n",
    "            logging.debug(f\"Q-learner won\")\n",
    "            q_learner.clear_previous_vars()\n",
    "            \n",
    "            return \"q_learner won\"\n",
    "        \n",
    "        is_q_learner = False\n",
    "    \n",
    "    else: #if the current player is the external agent\n",
    "        move_to_apply = external_agent(nim) \n",
    "        logging.debug(f\" Agent move to apply: {move_to_apply}\")\n",
    "        nim.nimming(move_to_apply)\n",
    "        is_q_learner = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_GAMES = 10\n",
    "logging.getLogger().setLevel(logging.INFO)\n",
    "\n",
    "def q_learner_strategy(nim_size) -> QLearner: #function to train the q_learner\n",
    "  #q_learner will play against dumb, random, optimizer\n",
    "    \n",
    "  q_learner_agent = QLearner(1, 0.3, 0.2) #change this later\n",
    "\n",
    "  for opponent in OPPONENTS:\n",
    "    for game in range(NUM_GAMES):\n",
    "        play_q_learning(nim_size, q_learner_agent, opponent)\n",
    "        logging.debug(f\" GAME FINISHED\")\n",
    "\n",
    "  return q_learner_agent\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:root:current_move - game not finished: (1, 3)\n",
      "DEBUG:root: Wanted move after player = q-learner: (1, 3), State before move: <1 3 5 7>\n",
      "DEBUG:root:move to apply: Nimply(row=1, num_objects=3)\n",
      "DEBUG:root: <<NIM>> after q-learner move: <1 0 5 7>\n",
      "DEBUG:root: Agent move to apply: Nimply(row=3, num_objects=1)\n",
      "DEBUG:root: REWARD: 0\n",
      "DEBUG:root:current_move - game not finished: (2, 4)\n",
      "DEBUG:root: Wanted move after player = q-learner: (2, 4), State before move: <1 0 5 6>\n",
      "DEBUG:root:move to apply: Nimply(row=2, num_objects=4)\n",
      "DEBUG:root: <<NIM>> after q-learner move: <1 0 1 6>\n",
      "DEBUG:root: Agent move to apply: Nimply(row=3, num_objects=1)\n",
      "DEBUG:root: REWARD: 0\n",
      "DEBUG:root:current_move - game not finished: (3, 1)\n",
      "DEBUG:root: Wanted move after player = q-learner: (3, 1), State before move: <1 0 1 5>\n",
      "DEBUG:root:move to apply: Nimply(row=3, num_objects=1)\n",
      "DEBUG:root: <<NIM>> after q-learner move: <1 0 1 4>\n",
      "DEBUG:root: Agent move to apply: Nimply(row=3, num_objects=1)\n",
      "DEBUG:root: REWARD: 0\n",
      "DEBUG:root:current_move - game not finished: (0, 1)\n",
      "DEBUG:root: Wanted move after player = q-learner: (0, 1), State before move: <1 0 1 3>\n",
      "DEBUG:root:move to apply: Nimply(row=0, num_objects=1)\n",
      "DEBUG:root: <<NIM>> after q-learner move: <0 0 1 3>\n",
      "DEBUG:root: Agent move to apply: Nimply(row=3, num_objects=1)\n",
      "DEBUG:root: REWARD: 0\n",
      "DEBUG:root:current_move - game not finished: (3, 1)\n",
      "DEBUG:root: Wanted move after player = q-learner: (3, 1), State before move: <0 0 1 2>\n",
      "DEBUG:root:move to apply: Nimply(row=3, num_objects=1)\n",
      "DEBUG:root: <<NIM>> after q-learner move: <0 0 1 1>\n",
      "DEBUG:root: Agent move to apply: Nimply(row=2, num_objects=1)\n",
      "DEBUG:root: REWARD: 1\n",
      "DEBUG:root:current_move - game not finished: (3, 1)\n",
      "DEBUG:root: Wanted move after player = q-learner: (3, 1), State before move: <0 0 0 1>\n",
      "DEBUG:root:move to apply: Nimply(row=3, num_objects=1)\n",
      "DEBUG:root: <<NIM>> after q-learner move: <0 0 0 0>\n",
      "DEBUG:root:Q-learner won\n",
      "DEBUG:root: GAME FINISHED\n",
      "DEBUG:root:current_move - game not finished: (2, 4)\n",
      "DEBUG:root: Wanted move after player = q-learner: (2, 4), State before move: <1 3 5 7>\n",
      "DEBUG:root:move to apply: Nimply(row=2, num_objects=4)\n",
      "DEBUG:root: <<NIM>> after q-learner move: <1 3 1 7>\n",
      "DEBUG:root: Agent move to apply: Nimply(row=3, num_objects=1)\n",
      "DEBUG:root: REWARD: 0\n",
      "DEBUG:root:current_move - game not finished: (3, 1)\n",
      "DEBUG:root: Wanted move after player = q-learner: (3, 1), State before move: <1 3 1 6>\n",
      "DEBUG:root:move to apply: Nimply(row=3, num_objects=1)\n",
      "DEBUG:root: <<NIM>> after q-learner move: <1 3 1 5>\n",
      "DEBUG:root: Agent move to apply: Nimply(row=3, num_objects=1)\n",
      "DEBUG:root: REWARD: 0\n",
      "DEBUG:root:current_move - game not finished: (1, 1)\n",
      "DEBUG:root: Wanted move after player = q-learner: (1, 1), State before move: <1 3 1 4>\n",
      "DEBUG:root:move to apply: Nimply(row=1, num_objects=1)\n",
      "DEBUG:root: <<NIM>> after q-learner move: <1 2 1 4>\n",
      "DEBUG:root: Agent move to apply: Nimply(row=3, num_objects=1)\n",
      "DEBUG:root: REWARD: 0\n",
      "DEBUG:root:current_move - game not finished: (2, 1)\n",
      "DEBUG:root: Wanted move after player = q-learner: (2, 1), State before move: <1 2 1 3>\n",
      "DEBUG:root:move to apply: Nimply(row=2, num_objects=1)\n",
      "DEBUG:root: <<NIM>> after q-learner move: <1 2 0 3>\n",
      "DEBUG:root: Agent move to apply: Nimply(row=3, num_objects=1)\n",
      "DEBUG:root: REWARD: 0\n",
      "DEBUG:root:current_move - game not finished: (0, 1)\n",
      "DEBUG:root: Wanted move after player = q-learner: (0, 1), State before move: <1 2 0 2>\n",
      "DEBUG:root:move to apply: Nimply(row=0, num_objects=1)\n",
      "DEBUG:root: <<NIM>> after q-learner move: <0 2 0 2>\n",
      "DEBUG:root: Agent move to apply: Nimply(row=1, num_objects=1)\n",
      "DEBUG:root: REWARD: 0\n",
      "DEBUG:root:current_move - game not finished: (3, 1)\n",
      "DEBUG:root: Wanted move after player = q-learner: (3, 1), State before move: <0 1 0 2>\n",
      "DEBUG:root:move to apply: Nimply(row=3, num_objects=1)\n",
      "DEBUG:root: <<NIM>> after q-learner move: <0 1 0 1>\n",
      "DEBUG:root: Agent move to apply: Nimply(row=1, num_objects=1)\n",
      "DEBUG:root: REWARD: 1\n",
      "DEBUG:root:current_move - game not finished: (3, 1)\n",
      "DEBUG:root: Wanted move after player = q-learner: (3, 1), State before move: <0 0 0 1>\n",
      "DEBUG:root:move to apply: Nimply(row=3, num_objects=1)\n",
      "DEBUG:root: <<NIM>> after q-learner move: <0 0 0 0>\n",
      "DEBUG:root:Q-learner won\n",
      "DEBUG:root: GAME FINISHED\n",
      "DEBUG:root:current_move - game not finished: (2, 1)\n",
      "DEBUG:root: Wanted move after player = q-learner: (2, 1), State before move: <1 3 5 7>\n",
      "DEBUG:root:move to apply: Nimply(row=2, num_objects=1)\n",
      "DEBUG:root: <<NIM>> after q-learner move: <1 3 4 7>\n",
      "DEBUG:root: Agent move to apply: Nimply(row=3, num_objects=1)\n",
      "DEBUG:root: REWARD: 0\n",
      "DEBUG:root:current_move - game not finished: (2, 3)\n",
      "DEBUG:root: Wanted move after player = q-learner: (2, 3), State before move: <1 3 4 6>\n",
      "DEBUG:root:move to apply: Nimply(row=2, num_objects=3)\n",
      "DEBUG:root: <<NIM>> after q-learner move: <1 3 1 6>\n",
      "DEBUG:root: Agent move to apply: Nimply(row=3, num_objects=1)\n",
      "DEBUG:root: REWARD: 0\n",
      "DEBUG:root:current_move - game not finished: (2, 1)\n",
      "DEBUG:root: Wanted move after player = q-learner: (2, 1), State before move: <1 3 1 5>\n",
      "DEBUG:root:move to apply: Nimply(row=2, num_objects=1)\n",
      "DEBUG:root: <<NIM>> after q-learner move: <1 3 0 5>\n",
      "DEBUG:root: Agent move to apply: Nimply(row=3, num_objects=1)\n",
      "DEBUG:root: REWARD: 0\n",
      "DEBUG:root:current_move - game not finished: (1, 1)\n",
      "DEBUG:root: Wanted move after player = q-learner: (1, 1), State before move: <1 3 0 4>\n",
      "DEBUG:root:move to apply: Nimply(row=1, num_objects=1)\n",
      "DEBUG:root: <<NIM>> after q-learner move: <1 2 0 4>\n",
      "DEBUG:root: Agent move to apply: Nimply(row=3, num_objects=1)\n",
      "DEBUG:root: REWARD: 0\n",
      "DEBUG:root:current_move - game not finished: (3, 1)\n",
      "DEBUG:root: Wanted move after player = q-learner: (3, 1), State before move: <1 2 0 3>\n",
      "DEBUG:root:move to apply: Nimply(row=3, num_objects=1)\n",
      "DEBUG:root: <<NIM>> after q-learner move: <1 2 0 2>\n",
      "DEBUG:root: Agent move to apply: Nimply(row=1, num_objects=1)\n",
      "DEBUG:root: REWARD: 0\n",
      "DEBUG:root:current_move - game not finished: (3, 2)\n",
      "DEBUG:root: Wanted move after player = q-learner: (3, 2), State before move: <1 1 0 2>\n",
      "DEBUG:root:move to apply: Nimply(row=3, num_objects=2)\n",
      "DEBUG:root: <<NIM>> after q-learner move: <1 1 0 0>\n",
      "DEBUG:root: Agent move to apply: Nimply(row=0, num_objects=1)\n",
      "DEBUG:root: REWARD: 1\n",
      "DEBUG:root:current_move - game not finished: (1, 1)\n",
      "DEBUG:root: Wanted move after player = q-learner: (1, 1), State before move: <0 1 0 0>\n",
      "DEBUG:root:move to apply: Nimply(row=1, num_objects=1)\n",
      "DEBUG:root: <<NIM>> after q-learner move: <0 0 0 0>\n",
      "DEBUG:root:Q-learner won\n",
      "DEBUG:root: GAME FINISHED\n",
      "DEBUG:root:current_move - game not finished: (3, 7)\n",
      "DEBUG:root: Wanted move after player = q-learner: (3, 7), State before move: <1 3 5 7>\n",
      "DEBUG:root:move to apply: Nimply(row=3, num_objects=7)\n",
      "DEBUG:root: <<NIM>> after q-learner move: <1 3 5 0>\n",
      "DEBUG:root: Agent move to apply: Nimply(row=2, num_objects=1)\n",
      "DEBUG:root: REWARD: 0\n",
      "DEBUG:root:current_move - game not finished: (2, 3)\n",
      "DEBUG:root: Wanted move after player = q-learner: (2, 3), State before move: <1 3 4 0>\n",
      "DEBUG:root:move to apply: Nimply(row=2, num_objects=3)\n",
      "DEBUG:root: <<NIM>> after q-learner move: <1 3 1 0>\n",
      "DEBUG:root: Agent move to apply: Nimply(row=1, num_objects=1)\n",
      "DEBUG:root: REWARD: 0\n",
      "DEBUG:root:current_move - game not finished: (1, 1)\n",
      "DEBUG:root: Wanted move after player = q-learner: (1, 1), State before move: <1 2 1 0>\n",
      "DEBUG:root:move to apply: Nimply(row=1, num_objects=1)\n",
      "DEBUG:root: <<NIM>> after q-learner move: <1 1 1 0>\n",
      "DEBUG:root: Agent move to apply: Nimply(row=0, num_objects=1)\n",
      "DEBUG:root: REWARD: 0\n",
      "DEBUG:root:current_move - game not finished: (2, 1)\n",
      "DEBUG:root: Wanted move after player = q-learner: (2, 1), State before move: <0 1 1 0>\n",
      "DEBUG:root:move to apply: Nimply(row=2, num_objects=1)\n",
      "DEBUG:root: <<NIM>> after q-learner move: <0 1 0 0>\n",
      "DEBUG:root: Agent move to apply: Nimply(row=1, num_objects=1)\n",
      "DEBUG:root: Wanted move after player = q-learner: None, State before move: <0 0 0 0>\n",
      "DEBUG:root: Q-learner lost\n",
      "DEBUG:root: GAME FINISHED\n",
      "DEBUG:root:current_move - game not finished: (3, 2)\n",
      "DEBUG:root: Wanted move after player = q-learner: (3, 2), State before move: <1 3 5 7>\n",
      "DEBUG:root:move to apply: Nimply(row=3, num_objects=2)\n",
      "DEBUG:root: <<NIM>> after q-learner move: <1 3 5 5>\n",
      "DEBUG:root: Agent move to apply: Nimply(row=2, num_objects=1)\n",
      "DEBUG:root: REWARD: 0\n",
      "DEBUG:root:current_move - game not finished: (1, 1)\n",
      "DEBUG:root: Wanted move after player = q-learner: (1, 1), State before move: <1 3 4 5>\n",
      "DEBUG:root:move to apply: Nimply(row=1, num_objects=1)\n",
      "DEBUG:root: <<NIM>> after q-learner move: <1 2 4 5>\n",
      "DEBUG:root: Agent move to apply: Nimply(row=3, num_objects=1)\n",
      "DEBUG:root: REWARD: 0\n",
      "DEBUG:root:current_move - game not finished: (3, 3)\n",
      "DEBUG:root: Wanted move after player = q-learner: (3, 3), State before move: <1 2 4 4>\n",
      "DEBUG:root:move to apply: Nimply(row=3, num_objects=3)\n",
      "DEBUG:root: <<NIM>> after q-learner move: <1 2 4 1>\n",
      "DEBUG:root: Agent move to apply: Nimply(row=2, num_objects=1)\n",
      "DEBUG:root: REWARD: 0\n",
      "DEBUG:root:current_move - game not finished: (2, 2)\n",
      "DEBUG:root: Wanted move after player = q-learner: (2, 2), State before move: <1 2 3 1>\n",
      "DEBUG:root:move to apply: Nimply(row=2, num_objects=2)\n",
      "DEBUG:root: <<NIM>> after q-learner move: <1 2 1 1>\n",
      "DEBUG:root: Agent move to apply: Nimply(row=1, num_objects=1)\n",
      "DEBUG:root: REWARD: 0\n",
      "DEBUG:root:current_move - game not finished: (0, 1)\n",
      "DEBUG:root: Wanted move after player = q-learner: (0, 1), State before move: <1 1 1 1>\n",
      "DEBUG:root:move to apply: Nimply(row=0, num_objects=1)\n",
      "DEBUG:root: <<NIM>> after q-learner move: <0 1 1 1>\n",
      "DEBUG:root: Agent move to apply: Nimply(row=1, num_objects=1)\n",
      "DEBUG:root: REWARD: 0\n",
      "DEBUG:root:current_move - game not finished: (3, 1)\n",
      "DEBUG:root: Wanted move after player = q-learner: (3, 1), State before move: <0 0 1 1>\n",
      "DEBUG:root:move to apply: Nimply(row=3, num_objects=1)\n",
      "DEBUG:root: <<NIM>> after q-learner move: <0 0 1 0>\n",
      "DEBUG:root: Agent move to apply: Nimply(row=2, num_objects=1)\n",
      "DEBUG:root: Wanted move after player = q-learner: None, State before move: <0 0 0 0>\n",
      "DEBUG:root: Q-learner lost\n",
      "DEBUG:root: GAME FINISHED\n",
      "DEBUG:root:current_move - game not finished: (3, 5)\n",
      "DEBUG:root: Wanted move after player = q-learner: (3, 5), State before move: <1 3 5 7>\n",
      "DEBUG:root:move to apply: Nimply(row=3, num_objects=5)\n",
      "DEBUG:root: <<NIM>> after q-learner move: <1 3 5 2>\n",
      "DEBUG:root: Agent move to apply: Nimply(row=2, num_objects=1)\n",
      "DEBUG:root: REWARD: 0\n",
      "DEBUG:root:current_move - game not finished: (1, 3)\n",
      "DEBUG:root: Wanted move after player = q-learner: (1, 3), State before move: <1 3 4 2>\n",
      "DEBUG:root:move to apply: Nimply(row=1, num_objects=3)\n",
      "DEBUG:root: <<NIM>> after q-learner move: <1 0 4 2>\n",
      "DEBUG:root: Agent move to apply: Nimply(row=2, num_objects=1)\n",
      "DEBUG:root: REWARD: 0\n",
      "DEBUG:root:current_move - game not finished: (3, 1)\n",
      "DEBUG:root: Wanted move after player = q-learner: (3, 1), State before move: <1 0 3 2>\n",
      "DEBUG:root:move to apply: Nimply(row=3, num_objects=1)\n",
      "DEBUG:root: <<NIM>> after q-learner move: <1 0 3 1>\n",
      "DEBUG:root: Agent move to apply: Nimply(row=2, num_objects=1)\n",
      "DEBUG:root: REWARD: 0\n",
      "DEBUG:root:current_move - game not finished: (0, 1)\n",
      "DEBUG:root: Wanted move after player = q-learner: (0, 1), State before move: <1 0 2 1>\n",
      "DEBUG:root:move to apply: Nimply(row=0, num_objects=1)\n",
      "DEBUG:root: <<NIM>> after q-learner move: <0 0 2 1>\n",
      "DEBUG:root: Agent move to apply: Nimply(row=2, num_objects=1)\n",
      "DEBUG:root: REWARD: 0\n",
      "DEBUG:root:current_move - game not finished: (2, 1)\n",
      "DEBUG:root: Wanted move after player = q-learner: (2, 1), State before move: <0 0 1 1>\n",
      "DEBUG:root:move to apply: Nimply(row=2, num_objects=1)\n",
      "DEBUG:root: <<NIM>> after q-learner move: <0 0 0 1>\n",
      "DEBUG:root: Agent move to apply: Nimply(row=3, num_objects=1)\n",
      "DEBUG:root: Wanted move after player = q-learner: None, State before move: <0 0 0 0>\n",
      "DEBUG:root: Q-learner lost\n",
      "DEBUG:root: GAME FINISHED\n",
      "DEBUG:root:current_move - game not finished: (3, 1)\n",
      "DEBUG:root: Wanted move after player = q-learner: (3, 1), State before move: <1 3 5 7>\n",
      "DEBUG:root:move to apply: Nimply(row=3, num_objects=1)\n",
      "DEBUG:root: <<NIM>> after q-learner move: <1 3 5 6>\n",
      "DEBUG:root: Agent move to apply: Nimply(row=3, num_objects=1)\n",
      "DEBUG:root: REWARD: 0\n",
      "DEBUG:root:current_move - game not finished: (3, 3)\n",
      "DEBUG:root: Wanted move after player = q-learner: (3, 3), State before move: <1 3 5 5>\n",
      "DEBUG:root:move to apply: Nimply(row=3, num_objects=3)\n",
      "DEBUG:root: <<NIM>> after q-learner move: <1 3 5 2>\n",
      "DEBUG:root: Agent move to apply: Nimply(row=2, num_objects=1)\n",
      "DEBUG:root: REWARD: 0\n",
      "DEBUG:root:current_move - game not finished: (3, 2)\n",
      "DEBUG:root: Wanted move after player = q-learner: (3, 2), State before move: <1 3 4 2>\n",
      "DEBUG:root:move to apply: Nimply(row=3, num_objects=2)\n",
      "DEBUG:root: <<NIM>> after q-learner move: <1 3 4 0>\n",
      "DEBUG:root: Agent move to apply: Nimply(row=2, num_objects=1)\n",
      "DEBUG:root: REWARD: 0\n",
      "DEBUG:root:current_move - game not finished: (1, 1)\n",
      "DEBUG:root: Wanted move after player = q-learner: (1, 1), State before move: <1 3 3 0>\n",
      "DEBUG:root:move to apply: Nimply(row=1, num_objects=1)\n",
      "DEBUG:root: <<NIM>> after q-learner move: <1 2 3 0>\n",
      "DEBUG:root: Agent move to apply: Nimply(row=2, num_objects=1)\n",
      "DEBUG:root: REWARD: 0\n",
      "DEBUG:root:current_move - game not finished: (2, 2)\n",
      "DEBUG:root: Wanted move after player = q-learner: (2, 2), State before move: <1 2 2 0>\n",
      "DEBUG:root:move to apply: Nimply(row=2, num_objects=2)\n",
      "DEBUG:root: <<NIM>> after q-learner move: <1 2 0 0>\n",
      "DEBUG:root: Agent move to apply: Nimply(row=1, num_objects=1)\n",
      "DEBUG:root: REWARD: 0\n",
      "DEBUG:root:current_move - game not finished: (0, 1)\n",
      "DEBUG:root: Wanted move after player = q-learner: (0, 1), State before move: <1 1 0 0>\n",
      "DEBUG:root:move to apply: Nimply(row=0, num_objects=1)\n",
      "DEBUG:root: <<NIM>> after q-learner move: <0 1 0 0>\n",
      "DEBUG:root: Agent move to apply: Nimply(row=1, num_objects=1)\n",
      "DEBUG:root: Wanted move after player = q-learner: None, State before move: <0 0 0 0>\n",
      "DEBUG:root: Q-learner lost\n",
      "DEBUG:root: GAME FINISHED\n",
      "DEBUG:root:current_move - game not finished: (2, 2)\n",
      "DEBUG:root: Wanted move after player = q-learner: (2, 2), State before move: <1 3 5 7>\n",
      "DEBUG:root:move to apply: Nimply(row=2, num_objects=2)\n",
      "DEBUG:root: <<NIM>> after q-learner move: <1 3 3 7>\n",
      "DEBUG:root: Agent move to apply: Nimply(row=3, num_objects=1)\n",
      "DEBUG:root: REWARD: 0\n",
      "DEBUG:root:current_move - game not finished: (3, 3)\n",
      "DEBUG:root: Wanted move after player = q-learner: (3, 3), State before move: <1 3 3 6>\n",
      "DEBUG:root:move to apply: Nimply(row=3, num_objects=3)\n",
      "DEBUG:root: <<NIM>> after q-learner move: <1 3 3 3>\n",
      "DEBUG:root: Agent move to apply: Nimply(row=1, num_objects=1)\n",
      "DEBUG:root: REWARD: 0\n",
      "DEBUG:root:current_move - game not finished: (2, 3)\n",
      "DEBUG:root: Wanted move after player = q-learner: (2, 3), State before move: <1 2 3 3>\n",
      "DEBUG:root:move to apply: Nimply(row=2, num_objects=3)\n",
      "DEBUG:root: <<NIM>> after q-learner move: <1 2 0 3>\n",
      "DEBUG:root: Agent move to apply: Nimply(row=3, num_objects=1)\n",
      "DEBUG:root: REWARD: 0\n",
      "DEBUG:root:current_move - game not finished: (1, 2)\n",
      "DEBUG:root: Wanted move after player = q-learner: (1, 2), State before move: <1 2 0 2>\n",
      "DEBUG:root:move to apply: Nimply(row=1, num_objects=2)\n",
      "DEBUG:root: <<NIM>> after q-learner move: <1 0 0 2>\n",
      "DEBUG:root: Agent move to apply: Nimply(row=3, num_objects=1)\n",
      "DEBUG:root: REWARD: 0\n",
      "DEBUG:root:current_move - game not finished: (0, 1)\n",
      "DEBUG:root: Wanted move after player = q-learner: (0, 1), State before move: <1 0 0 1>\n",
      "DEBUG:root:move to apply: Nimply(row=0, num_objects=1)\n",
      "DEBUG:root: <<NIM>> after q-learner move: <0 0 0 1>\n",
      "DEBUG:root: Agent move to apply: Nimply(row=3, num_objects=1)\n",
      "DEBUG:root: Wanted move after player = q-learner: None, State before move: <0 0 0 0>\n",
      "DEBUG:root: Q-learner lost\n",
      "DEBUG:root: GAME FINISHED\n",
      "DEBUG:root:current_move - game not finished: (2, 3)\n",
      "DEBUG:root: Wanted move after player = q-learner: (2, 3), State before move: <1 3 5 7>\n",
      "DEBUG:root:move to apply: Nimply(row=2, num_objects=3)\n",
      "DEBUG:root: <<NIM>> after q-learner move: <1 3 2 7>\n",
      "DEBUG:root: Agent move to apply: Nimply(row=3, num_objects=1)\n",
      "DEBUG:root: REWARD: 0\n",
      "DEBUG:root:current_move - game not finished: (3, 3)\n",
      "DEBUG:root: Wanted move after player = q-learner: (3, 3), State before move: <1 3 2 6>\n",
      "DEBUG:root:move to apply: Nimply(row=3, num_objects=3)\n",
      "DEBUG:root: <<NIM>> after q-learner move: <1 3 2 3>\n",
      "DEBUG:root: Agent move to apply: Nimply(row=1, num_objects=1)\n",
      "DEBUG:root: REWARD: 0\n",
      "DEBUG:root:current_move - game not finished: (3, 2)\n",
      "DEBUG:root: Wanted move after player = q-learner: (3, 2), State before move: <1 2 2 3>\n",
      "DEBUG:root:move to apply: Nimply(row=3, num_objects=2)\n",
      "DEBUG:root: <<NIM>> after q-learner move: <1 2 2 1>\n",
      "DEBUG:root: Agent move to apply: Nimply(row=1, num_objects=1)\n",
      "DEBUG:root: REWARD: 0\n",
      "DEBUG:root:current_move - game not finished: (0, 1)\n",
      "DEBUG:root: Wanted move after player = q-learner: (0, 1), State before move: <1 1 2 1>\n",
      "DEBUG:root:move to apply: Nimply(row=0, num_objects=1)\n",
      "DEBUG:root: <<NIM>> after q-learner move: <0 1 2 1>\n",
      "DEBUG:root: Agent move to apply: Nimply(row=2, num_objects=1)\n",
      "DEBUG:root: REWARD: 0\n",
      "DEBUG:root:current_move - game not finished: (2, 1)\n",
      "DEBUG:root: Wanted move after player = q-learner: (2, 1), State before move: <0 1 1 1>\n",
      "DEBUG:root:move to apply: Nimply(row=2, num_objects=1)\n",
      "DEBUG:root: <<NIM>> after q-learner move: <0 1 0 1>\n",
      "DEBUG:root: Agent move to apply: Nimply(row=1, num_objects=1)\n",
      "DEBUG:root: REWARD: 1\n",
      "DEBUG:root:current_move - game not finished: (3, 1)\n",
      "DEBUG:root: Wanted move after player = q-learner: (3, 1), State before move: <0 0 0 1>\n",
      "DEBUG:root:move to apply: Nimply(row=3, num_objects=1)\n",
      "DEBUG:root: <<NIM>> after q-learner move: <0 0 0 0>\n",
      "DEBUG:root:Q-learner won\n",
      "DEBUG:root: GAME FINISHED\n",
      "DEBUG:root:current_move - game not finished: (3, 2)\n",
      "DEBUG:root: Wanted move after player = q-learner: (3, 2), State before move: <1 3 5 7>\n",
      "DEBUG:root:move to apply: Nimply(row=3, num_objects=2)\n",
      "DEBUG:root: <<NIM>> after q-learner move: <1 3 5 5>\n",
      "DEBUG:root: Agent move to apply: Nimply(row=2, num_objects=1)\n",
      "DEBUG:root: REWARD: 0\n",
      "DEBUG:root:current_move - game not finished: (0, 1)\n",
      "DEBUG:root: Wanted move after player = q-learner: (0, 1), State before move: <1 3 4 5>\n",
      "DEBUG:root:move to apply: Nimply(row=0, num_objects=1)\n",
      "DEBUG:root: <<NIM>> after q-learner move: <0 3 4 5>\n",
      "DEBUG:root: Agent move to apply: Nimply(row=3, num_objects=1)\n",
      "DEBUG:root: REWARD: 0\n",
      "DEBUG:root:current_move - game not finished: (1, 2)\n",
      "DEBUG:root: Wanted move after player = q-learner: (1, 2), State before move: <0 3 4 4>\n",
      "DEBUG:root:move to apply: Nimply(row=1, num_objects=2)\n",
      "DEBUG:root: <<NIM>> after q-learner move: <0 1 4 4>\n",
      "DEBUG:root: Agent move to apply: Nimply(row=2, num_objects=1)\n",
      "DEBUG:root: REWARD: 0\n",
      "DEBUG:root:current_move - game not finished: (3, 1)\n",
      "DEBUG:root: Wanted move after player = q-learner: (3, 1), State before move: <0 1 3 4>\n",
      "DEBUG:root:move to apply: Nimply(row=3, num_objects=1)\n",
      "DEBUG:root: <<NIM>> after q-learner move: <0 1 3 3>\n",
      "DEBUG:root: Agent move to apply: Nimply(row=2, num_objects=1)\n",
      "DEBUG:root: REWARD: 0\n",
      "DEBUG:root:current_move - game not finished: (3, 1)\n",
      "DEBUG:root: Wanted move after player = q-learner: (3, 1), State before move: <0 1 2 3>\n",
      "DEBUG:root:move to apply: Nimply(row=3, num_objects=1)\n",
      "DEBUG:root: <<NIM>> after q-learner move: <0 1 2 2>\n",
      "DEBUG:root: Agent move to apply: Nimply(row=2, num_objects=1)\n",
      "DEBUG:root: REWARD: 0\n",
      "DEBUG:root:current_move - game not finished: (3, 2)\n",
      "DEBUG:root: Wanted move after player = q-learner: (3, 2), State before move: <0 1 1 2>\n",
      "DEBUG:root:move to apply: Nimply(row=3, num_objects=2)\n",
      "DEBUG:root: <<NIM>> after q-learner move: <0 1 1 0>\n",
      "DEBUG:root: Agent move to apply: Nimply(row=1, num_objects=1)\n",
      "DEBUG:root: REWARD: 1\n",
      "DEBUG:root:current_move - game not finished: (2, 1)\n",
      "DEBUG:root: Wanted move after player = q-learner: (2, 1), State before move: <0 0 1 0>\n",
      "DEBUG:root:move to apply: Nimply(row=2, num_objects=1)\n",
      "DEBUG:root: <<NIM>> after q-learner move: <0 0 0 0>\n",
      "DEBUG:root:Q-learner won\n",
      "DEBUG:root: GAME FINISHED\n"
     ]
    }
   ],
   "source": [
    "strat = q_learner_strategy(4)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "def play_nim(q_learner: QLearner, opponent: Callable):\n",
    "\n",
    "    result = play_q_learning(4, q_learner, dumb)\n",
    "    print(result)\n",
    "\n",
    "# Create evoultion agent\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "q_learner lost\n"
     ]
    }
   ],
   "source": [
    "play_nim(strat, optimal_startegy)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 64-bit (microsoft store)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "8f2b8f7d7da3b55c8640ff0ad5b752ba61ffdffe564a4378c820bcd9964834b8"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
